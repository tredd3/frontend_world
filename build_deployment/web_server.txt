//Web server - tomcat, apache(httpd), nginix(master and slave architecture)
httpd is the Apache HyperText Transfer Protocol (HTTP) server program. It is designed to be run as a standalone
daemon process(runs in background, no user interaction required).
When used like this it will create a pool of child processes or threads to handle requests.
https://cloud.google.com/compute/docs/tutorials/basic-webserver-apache
inorder to make a computer webserver u need to install a software
When the browser requests the file via HTTP. When the request reaches the correct web server (hardware), the HTTP server(software) accepts request, 
finds the requested document (if it doesn't then a�404�response is returned), and sends it back to the browser, also through HTTP.
static web server - the server sends its hosted files "as-is" to your browser
dynamic web server(server side rendering) -  static web server + dynamic web content generation using 
CGI scripts(scripts which use CGI for info exchange with http server,i.e via env variables specified by CGI) 
https://en.wikipedia.org/wiki/Common_Gateway_Interface
We call it "dynamic" because the application server updates the hosted files before sending them to your browser via the HTTP server.
app server - for api responses 

/Using CGI scripts
A web server allows its owner to configure which URLs shall be handled by which CGI scripts.
This is usually done by marking a new directory within the document collection as containing CGI 
scripts — its name is often cgi-bin. For example, /usr/local/apache/htdocs/cgi-bin could be designated 
as a CGI directory on the web server. When a Web browser requests a URL that points to a file within the 
CGI directory (e.g., http://example.com/cgi-bin/printenv.pl/with/additional/path?and=a&query=string), 
then, instead of simply sending that file (/usr/local/apache/htdocs/cgi-bin/printenv.pl) to the Web browser, 
the HTTP server runs the specified script and passes the output of the script to the Web browser.
That is, anything that the script sends to standard output is passed to the Web client instead of being shown on-screen in a terminal window.


//NGINIX - reverse proxy, caching, load balancing
**REVERSE PROXY is a type of proxy server that retrieves resources on behalf of a client from one or more servers. 
These resources are then returned to the client, appearing as if they originated from the proxy server itself.
**CACHING
What is application cache - When a slower external data source is used, frequently accessed data is often moved temporarily to a faster storage that is closer to the application 
Cache is used to reduce the time needed to access data stored outside of the application�s main memory space.
Why cache? - performance, speed, decrease latency , increase throughput and  also  reduce load on the main servers
for how much Time?- configurable
kind of data in  cache - Configuration settings, Reusable results of compute-intensive functions, static content(HTML,JS,CSS), DBMS data(replies to expensive queries from the DBMS),
API responses
An application component uses the API to make requests for service from other components, whether inside (e.g. in a micro-services architecture) or outside the application itself. 
Every request will hit the cache and  the response is served. If the response is not present then main memory servers will be hit, response is stored in the cache and then it is served

//why NGINIX is used for large enterprises
*easy installation and maintenance
*performance - reduces wait time because of load balancing and caching
*scalabilty - can handle huge number of concurrent requests
*on the fly upgrades - no need to bring down your server
*Nginx uses an asynchronous event-driven approach, rather than threads, to handle requests hence providing more predictable performance under high loads
*HTTP/2 and wbsockets support
*URL rewriting and redirection
URL rewriting/mapping - The technique adds a layer of abstraction between the files used to generate a web page and the URL that is presented to the outside world.
http://example.com/w/index.php?title=Page_title
but can be rewritten as:
http://example.com/wiki/Page_title


//configuring NGINIX (nginx.conf filename) - https://www.freecodecamp.org/news/the-nginx-handbook/
nginx -t //to validate configuration file for syntax mistakes
The way NGINX works is it reads the configuration file once and keeps working based on that.
If you update the configuration file, then you'll have to instruct NGINX explicitly to reload the configuration file.
There are two ways to do that :
1.You can restart the NGINX service by executing the sudo systemctl restart nginx command.
2.You can dispatch a reload signal to NGINX by executing the sudo nginx -s reload command (preferred)
The -s option is used for dispatching various signals to NGINX. The available signals are stop, quit, reload and reopen
Once you've reloaded the configuration file by executing the nginx -s reload command, you can see it in action by sending a simple get request to the server:
either from browser | or from terminal curl -i http://<server_name>
Simple Directives -  terminated by semicolons
Block directives - similar to simple directives, except that instead of ending with semicolons they end with a pair of curly braces { }
enclosing additional instructions.
A block directive capable of containing other directives inside it is called a context, that is events, http and so on. There are four core contexts in NGINX:
events { } – The events context is used for setting global configuration regarding how NGINX is going to handle requests on a general level. There can be only one events context in a valid configuration file.
http { } – Evident by the name, http context is used for defining configuration regarding how the server is going to handle HTTP and HTTPS requests, specifically. There can be only one http context in a valid configuration file.
server { } – The server context is nested inside the http context and used for configuring specific virtual servers within a single host. There can be multiple server contexts in a valid configuration file nested inside the http context. 
Each server context is considered a virtual host.
main – The main context is the configuration file itself. Anything written outside of the three previously mentioned contexts is on the main context.
location context - usually nested inside server blocks. There can be multiple location contexts within a server context.
by writing location /agatha, you're telling NGINX to match any URI starting with "agatha".  This kind of match is called a prefix match.
i.e you'll get the same response for 
To perform an exact match, you'll have to update the code as follows:
http {

    server {

        listen 80;
        server_name nginx-handbook.test;

        #prefix match
        # responds for http://nginx-handbook.test/agatha-christie and http://nginx-handbook.test/agatha anything that starts with agatha
        location /agatha {
            return 200 "Miss Marple.\nHercule Poirot.\n";
        }

        #exact match
        #only responds for http://nginx-handbook.test/agatha-christie 
        location = /agatha {
            return 200 "Miss Marple.\nHercule Poirot.\n";
        }

        #regex match case sensitive - more priority compared to prefix match
        # but the final type of match in NGINX is a preferential prefix match. To turn a prefix match into a preferential one, you need to include the ^~ modifier before the location URI:
        #http://nginx-handbook.test/agatha8 but not http://nginx-handbook.test/Agatha8
        location ~ /agatha[0-9] {
        	return 200 "Miss Marple.\nHercule Poirot.\n";
        }

        #regex match and case insensitive
        #http://nginx-handbook.test/agatha8 and http://nginx-handbook.test/Agatha8
        location ~* /agatha[0-9] {
        	return 200 "Miss Marple.\nHercule Poirot.\n";
        }

        #preferential prefix match - final priority
        location ^~ /Agatha8 {
        	return 200 "prefix matched.\n";
        }

        # redirect: If you visit http://nginx-handbook.test/about_page from a browser, you'll see that the URL will automatically change to http://nginx-handbook.test/about.html.
        location = /about_page {
          return 307 /about.html;
        }

        #rewrite directive, however, works a little differently. It changes the URI internally, without letting the user know.
        #if you send a request to http://nginx-handbook/about_page URI, you'll get a 200 response code and the HTML code for about.html file in response:
        #When a rewrite happens, the server context gets re-evaluated by NGINX. So, a rewrite is a more expensive operation than a redirect.
        rewrite /about_page /about.html;

    }

}

3 types of variables: String Integer Boolean
set $<variable_name> <variable_value>;
embedded variables : present internally in NGINX
return 200 "Host - $host\nURI - $uri\nArgs - $args\n";  #all these 3 variables are part of http core module
For a variable to be accessible in the configuration, NGINX has to be built with the module embedding the variable
# curl http://nginx-handbook.test/user?name=Farhan
# Host - nginx-handbook.test
# URI - /user
# Args - name=Farhan

Logging in NGINX
By default, NGINX's log files are located inside /var/log/nginx/access.log and var/log/nginx/error.log
http {

    include /etc/nginx/mime.types;

    server {

        listen 80;
        server_name nginx-handbook.test;
        #There are eight levels of error messages. set the minimum error level to warn. This way you won't have to look at unnecessary entries in the error log.
        error_log /var/log/error.log warn;
        
        location / {
            # logged to a default file /var/log/nginx/access.log
            return 200 "this will be logged to the default file.\n";
        }
        
        location = /admin {
            access_log /var/logs/nginx/admin.log;
            
            return 200 "this will be logged in a separate file.\n";
        }
        
        location = /no_logging {
            access_log off;
            
            return 200 "this will not be logged.\n";
        }
    }
}

#reverse proxy
http {
    listen 80;
    server_name nginx-handbook.test

    location / {
        proxy_pass http://localhost:3000;
        #below 3 are needed for forwarding websocket Connection to backend
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade; #proxy_set_header directive used for setting headers on an ongoing request to the backend server.
        proxy_set_header Connection 'upgrade';
    }
}

#load balancing - nginx automatically balances the load using weighted round-robin balancing method.
u can change the load balancing algorithm 
An upstream context in NGINX is a collection of servers that can be treated as a single backend.
http {

    upstream backend_servers {
        #requests from the same client will always be passed to the same server except when this server is unavailable
        ip_hash;

        server localhost:3001;
        server localhost:3002;
        server localhost:3003;
    }

    server {

        listen 80;
        server_name nginx-handbook.test;

        location / {
            proxy_pass http://backend_servers;
        }
    }
}

#Optimize NGINX for Maximum Performance
1)total connections handling
*worker process - no of workers process nginix will spawn to handle connections
The worker processes are asynchronous in nature. This means that they will process incoming requests as fast as the hardware can.
increasing the number of worker processes doesn't mean better performance. optimal number of worker processes = number of CPU cores.
worker_processes auto; #NGINX will set the number of processes based on the number of CPUs automatically
*worker connections - max connections per worker/ this decides how many users nginix can serve simultaneously
Just like the number of worker processes, this number is also related to the number of your CPU core and the number of files your operating system is allowed to open per core.
2)caching
http {

    include /env/nginx/mime.types;

    #compression technique
    gzip on;
    gzip_comp_level 3;

    gzip_types text/css text/javascript;

    server {

        listen 80;
        server_name nginx-handbook.test;

        root /srv/nginx-handbook-demo/static-demo;
        
        gzip on;
        gzip_comp_level 3;

        gzip_types text/css text/javascript;
        
        location ~* \.(css|js|jpg)$ {
            access_log off;
            #adding response headers to instruct browser to cache the content and its validity
            add_header Cache-Control public;
            add_header Pragma public;
            add_header Vary Accept-Encoding;
            expires 1M;
        }
    }
}
3)file compression
*gzip - better compression and decompression speed for dynamic/on the fly compression
*brotli - better compression ratio - for static assets
By default, NGINX compresses HTML responses. To compress other file formats, you'll have to pass them as parameters to the gzip_types directive.
By writing gzip_types text/css text/javascript;  you're telling NGINX to compress any file with the mime types of text/css and text/javascript
Configuring compression in NGINX is not enough. The client has to ask for the compressed response instead of the uncompressed responses. 
by sending "Accept-Encoding: gzip" in the request header

//why we need proxy or reverse proxy?
*security - no direct access to any of your file transfer servers. Everyone has to pass through the reverse proxy. 
When that happens, you can focus monitoring over what goes in and goes out through the reverse proxy
*single point of access control(gateway) - instead of specifying on every single server what IP addresses 
should be allowed to connect, you can simply create a set of IP access rules on your reverse proxy
*By moving your servers into your internal network and deploying a reverse proxy to control access, 
you can provide better security to those credentials and also sensitive data
*All your servers can be placed in your internal network and they can serve both your internal and external clients.
i.e you can share sensitive information with external partners even without putting the information
 on the DMZ(internet) or granting direct access to your back-end servers to external clients.
*load balancing - uses algorithm like round robin to distribute the load among the servers in the cluster.
*caching - As the cache system of the proxy server is very good, when you access any websites using 
a proxy server, it is having the chance to store your desired data in their cache system.
 And as a result, you can access them whenever you want


//Proxy servers and tunneling 
*A proxy can be on the user's local computer, or anywhere between the user's computer and a destination server on the Internet.
**There are two types of proxies: forward proxies (or tunnel, or gateway) and reverse proxies (used to control and protect access to a server for load-balancing, authentication, decryption or caching).
Forward proxies can hide the identities of clients whereas reverse proxies can hide the identities of servers
**functions of proxy server: 
caching (the cache can be public or private, like the browser cache)
filtering (like an antivirus scan or parental controls)
load balancing (to allow multiple servers to serve the different requests)
authentication (to control access to different resources)
logging (allowing the storage of historical information)
Tunneling - transmits private network data and protocol information through public network by encapsulating the data.
The HTTP protocol specifies a request method called CONNECT
It starts two-way communications with the requested resource and can be used to open a tunnel. This is how a client behind an HTTP proxy can access websites using SSL (i.e. HTTPS, port 443)
